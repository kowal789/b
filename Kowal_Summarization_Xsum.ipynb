{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCEqCJTCAnT8"
   },
   "source": [
    "## Summarization\n",
    "\n",
    "we try to summarize on the [xsum](https://huggingface.co/datasets/knkarthick/xsum) dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yha8cgnwAngF",
    "outputId": "6e2b55d3-fe1a-4ef4-ac9a-6c877fe01b47",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/gabriel/.local/lib/python3.10/site-packages (0.1.99)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in /home/gabriel/.local/lib/python3.10/site-packages (4.37.1)\n",
      "Requirement already satisfied: filelock in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (4.62.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: requests in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (1.24.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.11 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers[torch]) (0.26.1)\n",
      "Requirement already satisfied: psutil in /home/gabriel/.local/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.18.1)\n",
      "Requirement already satisfied: jinja2 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/gabriel/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.11->transformers[torch]) (12.3.101)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers[torch]) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers[torch]) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/gabriel/.local/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: zstandard in /home/gabriel/.local/lib/python3.10/site-packages (0.22.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sacrebleu in /home/gabriel/.local/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/gabriel/.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from sacrebleu) (4.8.0)\n",
      "Requirement already satisfied: portalocker in /home/gabriel/.local/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: regex in /home/gabriel/.local/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gabriel/.local/lib/python3.10/site-packages (from sacrebleu) (1.24.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu) (0.4.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge_score in /home/gabriel/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/gabriel/.local/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/gabriel/.local/lib/python3.10/site-packages (from rouge_score) (1.24.0)\n",
      "Requirement already satisfied: nltk in /home/gabriel/.local/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gabriel/.local/lib/python3.10/site-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/gabriel/.local/lib/python3.10/site-packages (from nltk->rouge_score) (4.62.1)\n",
      "Requirement already satisfied: joblib in /home/gabriel/.local/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/gabriel/.local/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: psutil in /home/gabriel/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gabriel/.local/lib/python3.10/site-packages (from accelerate) (1.24.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/gabriel/.local/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/gabriel/.local/lib/python3.10/site-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/gabriel/.local/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: typing-extensions in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/gabriel/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/gabriel/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.62.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/gabriel/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/gabriel/.local/lib/python3.10/site-packages (4.37.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (1.24.0)\n",
      "Requirement already satisfied: requests in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: filelock in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/gabriel/.local/lib/python3.10/site-packages (from transformers) (4.62.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/gabriel/.local/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: xxhash in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: pandas in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (1.24.0)\n",
      "Requirement already satisfied: filelock in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (4.62.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/gabriel/.local/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gabriel/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/gabriel/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gabriel/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gabriel/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gabriel/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gabriel/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gabriel/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gabriel/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gabriel/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jupyterlab_widgets in /home/gabriel/.local/lib/python3.10/site-packages (3.0.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/gabriel/.local/lib/python3.10/site-packages (4.62.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install 'transformers[torch]'\n",
    "!pip install zstandard\n",
    "!pip install sacrebleu\n",
    "!pip install rouge_score\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install jupyterlab_widgets\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YGRPEXpYFPz",
    "outputId": "5d78c550-d1c8-44d1-d422-4b995ac3fa1f"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "453EkI9mAp4d",
    "outputId": "d6238701-fbf7-445c-d0ab-0a60cc02ef1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "#!pip install tensorflow_probability==0.12.2\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edpTO-VdAt2p",
    "outputId": "7a70917b-adf2-409b-db7b-ccecbca774d3"
   },
   "outputs": [],
   "source": [
    "# Load the XSum dataset\n",
    "ds = load_dataset(\"knkarthick/xsum\", split=None)\n",
    "print \n",
    "# Access each split directly\n",
    "train_set = ds['train']\n",
    "valid_set = ds['validation']\n",
    "test_set = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvZJQPjlxS8e",
    "outputId": "81672c6d-6e6c-416b-a1df-5a42a50c4781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYfRg1s4LIJd",
    "outputId": "c0e19c13-3990-4e15-dcf8-2441b57d9d97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 40143035,\n",
       " 'dialogue': 'A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"',\n",
       " 'summary': 'Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "U-_1QMm5LGqc",
    "outputId": "8e3e68a7-c766-4636-b478-f0d3771731b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = ds[\"train\"][1][\"dialogue\"][:2000]\n",
    "sample_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD7e6nKlWYBe"
   },
   "source": [
    "#Generating a baseline summary taking the first 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "zhV-fLoHWuRx"
   },
   "outputs": [],
   "source": [
    "def get_baseline_summary(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return ' '.join(sentences[:num_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8Yt65nSMLOSo"
   },
   "outputs": [],
   "source": [
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "_asHs_pYXAbn"
   },
   "outputs": [],
   "source": [
    "summaries[\"baseline\"] = get_baseline_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DREX1Yk0Bzwz"
   },
   "source": [
    "### GPT2\n",
    "Can implement a summarization by using the text-generation pipeline but appending a \"TL:DR\" at the end of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "7xM7WQBMBlJC",
    "outputId": "bf4d5877-3a3b-4479-f64e-4a5a80a4b988"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAt one time, three buses were parked side-by-side in the Carpark side of Belfast, in the wake of the horrific event.\\nThe buses were to be replaced by buses from the other two hotels in the area.\\nThey were then to leave Belfast, the Northern Ireland Transport Authority said in a statement.\\nThe vans were supposed to have arrived at Derry, but the fire that engulfed them was brought to the attention of the Northern Ireland fire service.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"gpt2\") #If gpt2-xl is too large use a small version\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\n",
    "summaries[\"gpt2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKDx9P99XX01"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lHMzrioCY_J"
   },
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "qlFUhSo0BSaK",
    "outputId": "1a709208-fc0b-4288-dae2-b90ccd092bb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 194. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a fire alarm went off at the Holiday Inn in Hope Street on Saturday .\\nguests were asked to leave the hotel .\\nthe two buses were parked side-by-side in the car park .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-small\") #If t5-large is too large use t5-small version\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "summaries[\"t5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJAqp0cQKXAj"
   },
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "L2kk4v13CcNb",
    "outputId": "da990331-8486-4a2f-8dc1-f1824058e673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST.\\nOne of the tour groups is from Germany, the other from China and Taiwan.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "summaries[\"bart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DevX5aBDTYxP",
    "outputId": "9c536bb1-908f-4e2b-8691-a62af676b5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.\n",
      "\n",
      "BASELINE\n",
      "A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel. As they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames. One of the tour groups is from Germany, the other from China and Taiwan.\n",
      "\n",
      "GPT2\n",
      "\n",
      "At one time, three buses were parked side-by-side in the Carpark side of Belfast, in the wake of the horrific event.\n",
      "The buses were to be replaced by buses from the other two hotels in the area.\n",
      "They were then to leave Belfast, the Northern Ireland Transport Authority said in a statement.\n",
      "The vans were supposed to have arrived at Derry, but the fire that engulfed them was brought to the attention of the Northern Ireland fire service.\n",
      "\n",
      "T5\n",
      "a fire alarm went off at the Holiday Inn in Hope Street on Saturday .\n",
      "guests were asked to leave the hotel .\n",
      "the two buses were parked side-by-side in the car park .\n",
      "\n",
      "BART\n",
      "Fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST.\n",
      "One of the tour groups is from Germany, the other from China and Taiwan.\n",
      "The driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(ds[\"train\"][1][\"summary\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SK4wFy9C1zU"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "How can we evaluate the performance of a summarization model?\n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy) Score**  is a precision based metric that measures how many words or n-grams present in the machine generated summaries also appear in the human reference summaries divided by the length of the generation. It penalises the appearance of too short summaries with a brevity penalty. [Sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu).\n",
    "\n",
    "**ROUGE (Recall Oriented Understudy for Gisting Evaluation) score** is the recall based metric that measures how many words/ngrams present in the human reference summaries were found in the machine generated summaries. Some sub-forms:\n",
    "\n",
    "1. ROUGE-N: Overlap of n-grams\n",
    "2. ROUGE-1: Overlap of unigrams.\n",
    "3. ROUGE-2: Oerlap of bigrams.\n",
    "4. ROUGE-L: Overlap of the Longest Common Subsequence\n",
    "\n",
    "To Do:\n",
    "1. How do you interpret a Bleu/ Rouge Score of 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeNQsraNCrij",
    "outputId": "9aca609f-7d5d-4829-ecdd-46edb106f51b"
   },
   "outputs": [],
   "source": [
    "bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9i_Oyfd6TtQA",
    "outputId": "c40e3afd-1f4c-431e-cdb7-09d8f1548177"
   },
   "outputs": [],
   "source": [
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DZYfYzvkmtwE",
    "outputId": "78803099-c991-44b5-c33c-45b179d37b9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = ds[\"train\"][1][\"summary\"]\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Aou4i_PMlQuh"
   },
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "for model_name in summaries:\n",
    "  bleu_metric.add(prediction=summaries[model_name], reference=[reference])\n",
    "  results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "  bleu_scores.append(results[\"score\"])\n",
    "  #results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "Ndmv6fceWAsh",
    "outputId": "41f842ed-bd15-48e6-f903-fe15feb1cf8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum  bleu\n",
       "baseline  0.179487  0.000000  0.102564   0.102564   0.0\n",
       "gpt2      0.185567  0.021053  0.103093   0.144330   0.0\n",
       "t5        0.280000  0.000000  0.160000   0.200000   0.0\n",
       "bart      0.142857  0.029412  0.085714   0.142857   0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    rouge_score = rouge_metric.compute()\n",
    "    result_dict = dict((rn, rouge_score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    bleu_metric.add(prediction=summaries[model_name], reference=[reference])\n",
    "    bleu_results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "    result_dict[\"bleu\"] = bleu_results[\"score\"]\n",
    "    records.append(result_dict)\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag7xSni3jWYU"
   },
   "source": [
    "BASELINE: some overlap of individual words, but bad performance on word pairs and no overlap (BLEU)\n",
    "\n",
    "GPT2: captured some bigrams (rouge2) but the bleu score indicates still no overlaps\n",
    "\n",
    "T5: best capturing in unigrams, but still no real performance in higher overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4N9V9knsEdY"
   },
   "source": [
    "## Finetuning a model for Summarization (BERT)\n",
    "\n",
    "#Discussion\n",
    "How would you fine tune a model for summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "eylE48_Buxrg"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Mm6CeOAgubVM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKfr0ksYnKw7"
   },
   "source": [
    "Truncation of input to 1024 characters and 161 target summary size due to the training data has 95% of data below 161 characters here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "s4uUtEeDtfx0"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_features(batch, tokenizer, max_input_length=1024, max_target_length=161):\n",
    "    input_encodings = tokenizer(batch['dialogue'], padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
    "    target_encodings = tokenizer(batch['summary'], padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
    "\n",
    "    return {\n",
    "    \"input_ids\": input_encodings.input_ids,\n",
    "    \"attention_mask\": input_encodings.attention_mask,\n",
    "    \"labels\": target_encodings.input_ids\n",
    "    }\n",
    "    #with tokenizer.as_target_tokenizer(): #as_target_tokenizer helps to differentiate between\n",
    "    #    target_encodings = tokenizer(batch[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    #return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "    #        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "    #        \"labels\": target_encodings[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4c58c819cec9467390979f88e0b83c41",
      "c42dc52891394d828b78bdfc1400a9e6",
      "136fb8d4c2514d9c809673cfba19c83d",
      "1a482faee07a4491ab496529f076ab79",
      "e61e186e6b75440f93564cdd68c7c873",
      "4a75d6018ef748bbb0fbde3c7e9d037b",
      "8c2d88f26ba44cd0923d47e7d63a80a9",
      "da99480d348b43c5af4594adb4eec1f1",
      "d3db4f2f030c46e3b2aad44bc4cd1a34",
      "6f207af3c0e64e8ebdc50a60249a5833",
      "3a6bf2695aa14f46aabce90aa615329a"
     ]
    },
    "id": "GKs4wApOv3-t",
    "outputId": "2ffade78-9d51-4f00-e8a2-8219452c7ef7"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01332235336303711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 204017,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e790818f0043f4908eb819860fbd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00971841812133789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 11327,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb48b49c1d046fb9d4b4832a9b630f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006766319274902344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 11333,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff7d288e1da4b7fa08247c297ba5300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out rows where either 'dialogue' or 'summary' is None\n",
    "filtered_ds = ds.filter(lambda x: x['dialogue'] is not None and x['summary'] is not None)\n",
    "\n",
    "# Applying the convert_data_to_features function to the filtered dataset\n",
    "dataset_xsum = filtered_ds.map(lambda batch: convert_data_to_features(batch, tokenizer), batched=True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_xsum.set_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iWNMp-olvaU3"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xSw2qaskvdZL"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output_folder', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10, push_to_hub=False,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "U1di_rRBva50"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_xsum[\"train\"],\n",
    "                  eval_dataset=dataset_xsum[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "6L1IRC5xvpcp",
    "outputId": "afa9a778-f99e-4ca3-8b0d-195ae149024b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12751' max='12751' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12751/12751 17:31:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.370914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.360295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>0.354789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>0.349558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.346127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.342745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.340163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.338319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.356600</td>\n",
       "      <td>0.336035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.334091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.332364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.330725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.356200</td>\n",
       "      <td>0.329806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.328250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.325550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.324824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.323350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.322267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.321091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.319683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.318894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.317971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>0.317293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.316307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.315857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12751, training_loss=0.47091856156645656, metrics={'train_runtime': 63066.8981, 'train_samples_per_second': 3.235, 'train_steps_per_second': 0.202, 'total_flos': 1.2439599326429184e+17, 'train_loss': 0.47091856156645656, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "gzS0XbwsyCc6"
   },
   "outputs": [],
   "source": [
    "test_dataset=dataset_xsum[\"test\"]\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def evaluate_summaries(test_dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"dialogue\",\n",
    "                               column_summary=\"summary\"):\n",
    "    article_batches = list(chunks(test_dataset,[column_text], batch_size))\n",
    "    target_batches = list(chunks(test_dataset,[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=161)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "    print(\"Score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to find max length of a field in a dataset split, handling None values\n",
    "#def find_max_length(dataset, field):\n",
    "#    return max(len(entry[field]) if entry[field] is not None else 0 for entry in dataset)\n",
    "\n",
    "\n",
    "# Calculate max length for 'dialogue' and 'summary' in each split\n",
    "#max_length_train_dialogue = find_max_length(ds['train'], 'dialogue')\n",
    "#max_length_train_summary = find_max_length(ds['train'], 'summary')\n",
    "\n",
    "#max_length_val_dialogue = find_max_length(ds['validation'], 'dialogue')\n",
    "#max_length_val_summary = find_max_length(ds['validation'], 'summary')\n",
    "\n",
    "#max_length_test_dialogue = find_max_length(ds['test'], 'dialogue')\n",
    "#max_length_test_summary = find_max_length(ds['test'], 'summary')\n",
    "\n",
    "# Print the results\n",
    "#print(\"Max length in 'train' - Dialogue:\", max_length_train_dialogue, \", Summary:\", max_length_train_summary)\n",
    "#print(\"Max length in 'validation' - Dialogue:\", max_length_val_dialogue, \", Summary:\", max_length_val_summary)\n",
    "#print(\"Max length in 'test' - Dialogue:\", max_length_test_dialogue, \", Summary:\", max_length_test_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries(dataset, metric, model, tokenizer, batch_size, device, column_text=\"dialogue\", column_summary=\"summary\"):\n",
    "    # Ensure dataset is filtered and preprocessed similarly to training/validation sets\n",
    "    dataset = dataset.filter(lambda x: x[column_text] is not None and x[column_summary] is not None)\n",
    "    dataset = dataset.map(lambda batch: convert_data_to_features(batch, tokenizer), batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "\n",
    "    # Rest of your existing evaluation logic...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "WctbFFgsvx2M"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009007692337036133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Filter",
       "rate": null,
       "total": 11333,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a137552156ec458d8c36e26fe4789b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005602836608886719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 11333,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f5a604bb8c4d13901912d189b5dc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge Score: None\n"
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate_summaries(test_dataset, rouge_metric, trainer.model, tokenizer,batch_size=2, device=device)\n",
    "# Debugging: print the rouge_score to see what it contains\n",
    "print(\"Rouge Score:\", rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "51SMRNFMz2Iq"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71961/766775647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrouge_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrouge_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"cnn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_71961/766775647.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrouge_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrouge_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"cnn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "rouge_dict = dict((rn, rouge_score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[f\"cnn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "XjUYDH4606yO",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## BLEU score\n",
    "How would you calculate the BLEU score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: More than 1,000 new one-bedroom flats in Wales have been built in the past year, according to a charity.\n",
      "Corresponding reference summary: There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Four people have appeared in court accused of beating a black man to death in Chicago.\n",
      "Corresponding reference summary: Four people accused of kidnapping and torturing a mentally disabled man in a \"racially motivated\" attack streamed on Facebook have been denied bail.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A \"fasting-mimicking\" diet has been shown to reverse the effects of diabetes, scientists say.\n",
      "Corresponding reference summary: The pancreas can be triggered to regenerate itself through a type of fasting diet, say US researchers.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Wendy Houvenaghel has accused British Cycling of \"cultural and governance failings\".\n",
      "Corresponding reference summary: A \"medal at any cost\" approach created a \"culture of fear\" at British Cycling, says former rider Wendy Houvenaghel.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:03,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Shares in BT jumped after the regulator Ofcom said it would not break up the company.\n",
      "Corresponding reference summary: The reaction from BT's investors told us much about media regulator Ofcom's ruling on the fate of Openreach, the BT subsidiary that provides much of the UK's broadband infrastructure.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:03,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Queen's University in Belfast is to cut the number of undergraduate and postgraduate places by 1,000.\n",
      "Corresponding reference summary: Queen's University Belfast is cutting 236 jobs and 290 student places due to a funding reduction.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:04,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The shadow home secretary has told the BBC she has \"many conversations\" with Tory backbenchers over the draft Brexit bill.\n",
      "Corresponding reference summary: The shadow foreign secretary has suggested Labour will continue to support legislation paving the way for Brexit as it passes through Parliament.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The death of a man who was sectioned at Caernarfon General Infirmary could have been caused by a visit by police, an inquest has heard.\n",
      "Corresponding reference summary: North Wales Police has been criticised at an inquest for sending an officer to speak to a hospital patient with paranoid schizophrenia.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The former chief scientist of Nasa has called on the Trump administration to make data on climate change available to the public.\n",
      "Corresponding reference summary: Limiting access to federal research would do an \"enormous disservice\" to the US and the world according to former Nasa chief scientist.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: World number one Justin Rose is one shot off the lead after the first round of the Arnold Palmer Invitational.\n",
      "Corresponding reference summary: Tiger Woods missed the cut at the Farmers Insurance Open, as England's Justin Rose maintained a one-shot lead.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:06,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Internet service providers in Pakistan have been told to block access to YouTube after the government lifted a ban on the site.\n",
      "Corresponding reference summary: Pakistan has unblocked the video sharing site, YouTube, more than three years after it was banned for posting a video deemed insulting to Islam.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:07,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Olympic gold medallist Liam Doull says it was \"unbelievable\" to win gold in the men's road race at Rio 2016.\n",
      "Corresponding reference summary: Owain Doull has won Wales' first gold of the 2016 Olympics as  he helped the Great Britain men's team pursuit defend their cycling title in Rio.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:08,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A couple have become the first couple to get married on skis.\n",
      "Corresponding reference summary: All images copyrighted.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:08,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: This year's Baftas awards have been held in London.\n",
      "Corresponding reference summary: The Bafta awards had laughter, passion and plenty of politics.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:09,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: South African police are investigating the murder of a DJ who was shot dead in the capital, Pretoria.\n",
      "Corresponding reference summary: South African police say four people have been arrested in connection with the murder of former actor on popular local TV series Generations.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:09,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Britain's Jessica Ennis-Hill missed out on a top-12 finish in the heptathlon at the Gotzis Hypo-Meeting.\n",
      "Corresponding reference summary: Jessica Ennis-Hill has fallen from fourth to eighth place after six events at the Hypo-Meeting in Gotzis.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:10,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The number of candidates standing in the general election in Northern Ireland has risen to 228.\n",
      "Corresponding reference summary: A total of 228 candidates will contest the Northern Ireland Assembly election next month - 48 fewer than last time.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:10,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A New Zealand teenager has been crowned Alternative Model of the Year.\n",
      "Corresponding reference summary: A philosophy student who wears vintage clothing has won a national contest for alternative models.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:11,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A man has appeared in court charged with breaching terror control orders, the BBC has learned.\n",
      "Corresponding reference summary: A suspected terrorist has been charged with breaching conditions imposed as part of the government's new terror monitoring powers.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:11,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: League One side Bury have signed former Shakers midfielder Ryan Brown on loan until the end of the season.\n",
      "Corresponding reference summary: League One side Bury have signed former Barnsley defender Reece Brown on a six-month contract.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:12,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Last year's Eurovision Song Contest was supposed to be a festival of peace and friendship between Ukraine and Russia.\n",
      "Corresponding reference summary: Performers from 42 countries strode down a long red carpet near Ukraine's parliament this week, as a curtain-raiser to this year's Eurovision Song Contest.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:13,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: South Africa's government has unveiled plans to raise the national minimum wage to 12,000 rand (Â£7,500).\n",
      "Corresponding reference summary: South Africa's government has proposed a national minimum wage of 3,500 rand ($242; Â£199) a month.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:13,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Azhar Ali hit a career-best half-century as Australia beat Pakistan by seven wickets on day two of the second Test in Melbourne.\n",
      "Corresponding reference summary: Azhar Ali's unbeaten 66 helped Pakistan to reach 142-4 against Australia on a rain-affected first day of the Boxing Day Test in Melbourne.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:14,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Russia says it will continue its air strikes on Islamic State and Jabhat Fateh al-Sham in Syria despite a US refusal to co-operate.\n",
      "Corresponding reference summary: Russia has said it will carry on bombing rebel-held eastern Aleppo in Syria, defying US demands to stop.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:15,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: US President-elect Donald Trump has announced a new wave of sanctions against Russia.\n",
      "Corresponding reference summary: The expulsion of 35 Russian diplomats from the US over the email hacking scandal has drawn a barrage of abuse from Moscow, which seems poised to respond in kind.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:15,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The search for a pilot who died in a Shoreham Air Show crash is continuing, the fire service has said.\n",
      "Corresponding reference summary: Fire crews are expected to leave the site of the Shoreham Airshow disaster later after spending nearly three weeks at the scene of the crash.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:16,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The Obamas have hosted a private party in New York, the White House has confirmed.\n",
      "Corresponding reference summary: White House officials will not say whether pop star Prince performed at a weekend party at the executive residence despite guests posting about it on social media.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:16,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: In our world of mental health problems, many of us are struggling to cope with the challenges we face.\n",
      "Corresponding reference summary: Approximately one in four of us will experience a mental health problem each year in England.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:17,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The US has suspended talks with Russia in Geneva aimed at improving the humanitarian situation in Syria.\n",
      "Corresponding reference summary: The US has said it is suspending talks with Russia over Syria, accusing Moscow of having \"failed to live up\" to its commitments under a ceasefire deal.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:17,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A video of children singing a song about a new vicar has gone viral.\n",
      "Corresponding reference summary: A church in West Yorkshire has recruited a new vicar following a video job advertisement sung by a choir of children.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:18,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A man has been killed in a knife attack on a train in the German city of St Gallen, police say.\n",
      "Corresponding reference summary: A 34-year-old woman who was injured in an attack by a knifeman on a Swiss train has died in hospital, police say.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:18,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A seal has been rescued from the sea after it was tangled in netting.\n",
      "Corresponding reference summary: A seal found tangled in nets on an Aberdeenshire beach has been returned to the sea.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:19,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Leeds City Council's John Roache has been elected as the new general secretary of the GMB union.\n",
      "Corresponding reference summary: Regional official Tim Roache has been elected to become the new general secretary of the GMB union.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:20,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Newcastle United midfielder Jonas Gutierrez says he wants to prove he can still be a Premier League player.\n",
      "Corresponding reference summary: Newcastle midfielder Jonas Gutierrez says he feels \"born again\" following his return to Premier League action after overcoming testicular cancer.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:20,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Olympic javelin champion Abdullah Hayayei has died after being struck by a metal pole during a training session.\n",
      "Corresponding reference summary: Para-athlete Abdullah Hayayei died after a metal throwing cage fell on him during training.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:21,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: An inquiry into undercover policing in England and Wales is to be launched by the home secretary, Lord Justice Pitchford has said.\n",
      "Corresponding reference summary: The public inquiry into undercover policing may \"expose both creditable and discreditable conduct\", chairman Lord Justice Pitchford has warned.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:22,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The world's tallest obelisk is being surveyed by the National Trust for the first time.\n",
      "Corresponding reference summary: A computer model of one of the world's tallest three-sided obelisks is being made to find out why it is falling apart.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:22,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Ginger-songwriter Lady Gaga has been working on the rest of her Super Bowl half-time show in Houston.\n",
      "Corresponding reference summary: Lady Gaga is leading the pitch invasion at Sunday's Super Bowl, where she'll perform the all-important half-time show.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:23,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A shop in Dundee has been closed after water flooded the roof of the building.\n",
      "Corresponding reference summary: John Lewis's Nottingham store is to remain closed longer than expected after 80,000 litres of hot water leaked from a ruptured heating pipe.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:23,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Sale Sharks director of rugby Steve Cotton says the club needs to \"rebuild\" its facilities if they are to compete at the highest level.\n",
      "Corresponding reference summary: New Sale Sharks chairman Fran Cotton has conceded they must work to restore their relationship with some fans.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:24,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: British number one Johanna Konta reached the quarter-finals of the Australian Open with a straight-set victory over American Heather Watson.\n",
      "Corresponding reference summary: British number one Johanna Konta reached the quarter-finals of the Bank of the West Classic in Stanford with a straight-set win over Julia Boserup.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:24,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Scientists in Brazil have identified a species of bat that carried the Ebola virus to the death of a four-year-old boy in 2014.\n",
      "Corresponding reference summary: The Ebola victim who is believed to have triggered the current outbreak - a two-year-old boy called Emile Ouamouno from Guinea - may have been infected by playing in a hollow tree housing a colony of bats, say scientists.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:25,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Plans to demolish a Grade ll pier on the Isle of Wight have been approved by councillors.\n",
      "Corresponding reference summary: Part of Colwyn Bay pier is to be dismantled after it collapsed into the sea, Conwy council has decided.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:25,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The head of one of England's worst-funded schools has warned parents of \"unthinkable\" cuts to funding.\n",
      "Corresponding reference summary: Pupils at seven schools could have shorter days after a trust head say he may have to cut hours to save money.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:26,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Human remains have been found on a slip road in West Mercia.\n",
      "Corresponding reference summary: More human remains have been found near a motorway slip road in Shropshire, police have said.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:27,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: University support staff in England are staging a 24-hour strike in a row over pay.\n",
      "Corresponding reference summary: University lecturers are due to start a two-day strike over pay, amid warnings other staff could join the dispute.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:27,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: Former Japan striker Kazuyoshi Miura has signed a new two-and-a-half-year contract with the club.\n",
      "Corresponding reference summary: Japanese footballer Kazuyoshi Miura has signed a one-year contract extension with Yokohama FC at the age of 48.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:28,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: The UK has voted to leave the European Union.\n",
      "Corresponding reference summary: Economic policymakers must now decide whether and how they should respond to the UK's vote to leave the EU.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:28,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: A health board has apologised to the families of patients who died after being treated like animals at a psychiatric ward.\n",
      "Corresponding reference summary: The quality of care on a scandal-hit ward for dementia patients may have contributed to at least seven deaths, BBC Wales can reveal.\n",
      "Processing batch of size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:29,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summary: US women's team-mate Hope Solo has been suspended for three matches after testing positive for a banned substance.\n",
      "Corresponding reference summary: The United States women's team goalkeeper Hope Solo has been suspended for 30 days by US Soccer following an incident during a training camp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# functional testing on a subset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_bleu_score(dataset, model, tokenizer, device, subset_size=100, batch_size=16, column_text=\"dialogue\", column_summary=\"summary\"):\n",
    "    # Select a subset of the dataset for testing\n",
    "    dataset_subset = dataset.select(range(subset_size))\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    for batch in tqdm(chunks(dataset_subset, batch_size)):\n",
    "        #debug\n",
    "        print(f\"Processing batch of size: {len(batch[column_text])}\")\n",
    "        \n",
    "        # Ensure batch[column_text] is a list of strings\n",
    "        input_texts = [text for text in batch[column_text] if isinstance(text, str)]\n",
    "        reference_texts = [ref for ref in batch[column_summary] if isinstance(ref, str)]\n",
    "\n",
    "        # Adjust batch size based on filtered data\n",
    "        actual_batch_size = len(input_texts)\n",
    "        if actual_batch_size == 0:\n",
    "            continue\n",
    "\n",
    "        # Tokenizing the input text\n",
    "        inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generating summaries\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=161, num_beams=8)\n",
    "        \n",
    "        # Decoding the generated summaries\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "\n",
    "        # Ensure that we have the same number of references and candidates\n",
    "        decoded_summaries = decoded_summaries[:actual_batch_size]\n",
    "        reference_texts = reference_texts[:actual_batch_size]\n",
    "\n",
    "        # Debugging: Print a sample summary and its reference\n",
    "        if len(decoded_summaries) > 0:\n",
    "            print(\"Sample generated summary:\", decoded_summaries[0])\n",
    "            print(\"Corresponding reference summary:\", batch[column_summary][0])\n",
    "\n",
    "        \n",
    "        # Adding to candidates and references\n",
    "        candidates.extend(decoded_summaries)\n",
    "        references.extend([[ref.split()] for ref in reference_texts])\n",
    "\n",
    "    # Ensure equal number of candidates and references\n",
    "    assert len(candidates) == len(references), \"Number of candidates and references must be equal\"\n",
    "\n",
    "    # Calculating BLEU score\n",
    "    bleu_score = corpus_bleu(references, [c.split() for c in candidates])\n",
    "    return bleu_score\n",
    "\n",
    "# Testing with a subset\n",
    "bleu_score_subset = calculate_bleu_score(ds[\"test\"], trainer.model, tokenizer, device, subset_size=100, batch_size=2)\n",
    "print(\"BLEU Score on Subset:\", bleu_score_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5667it [1:17:29,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.09534446192887946\n"
     ]
    }
   ],
   "source": [
    "# evaluating real score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_bleu_score(dataset, model, tokenizer, device, subset_size=100, batch_size=16, column_text=\"dialogue\", column_summary=\"summary\"):\n",
    "    # Select a subset of the dataset for testing\n",
    "    #dataset_subset = dataset.select(range(subset_size))\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    for batch in tqdm(chunks(dataset, batch_size)):\n",
    "        #debug\n",
    "        #print(f\"Processing batch of size: {len(batch[column_text])}\")\n",
    "        \n",
    "        # Ensure batch[column_text] is a list of strings\n",
    "        input_texts = [text for text in batch[column_text] if isinstance(text, str)]\n",
    "        reference_texts = [ref for ref in batch[column_summary] if isinstance(ref, str)]\n",
    "\n",
    "        # Adjust batch size based on filtered data\n",
    "        actual_batch_size = len(input_texts)\n",
    "        if actual_batch_size == 0:\n",
    "            continue\n",
    "\n",
    "        # Tokenizing the input text\n",
    "        inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generating summaries\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=161, num_beams=8)\n",
    "        \n",
    "        # Decoding the generated summaries\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "\n",
    "        # Ensure that we have the same number of references and candidates\n",
    "        decoded_summaries = decoded_summaries[:actual_batch_size]\n",
    "        reference_texts = reference_texts[:actual_batch_size]\n",
    "\n",
    "        # Debugging: Print a sample summary and its reference\n",
    "        #if len(decoded_summaries) > 0:\n",
    "        #    print(\"Sample generated summary:\", decoded_summaries[0])\n",
    "        #    print(\"Corresponding reference summary:\", batch[column_summary][0])\n",
    "\n",
    "        \n",
    "        # Adding to candidates and references\n",
    "        candidates.extend(decoded_summaries)\n",
    "        references.extend([[ref.split()] for ref in reference_texts])\n",
    "\n",
    "    # Ensure equal number of candidates and references\n",
    "    assert len(candidates) == len(references), \"Number of candidates and references must be equal\"\n",
    "\n",
    "    # Calculating BLEU score\n",
    "    bleu_score = corpus_bleu(references, [c.split() for c in candidates])\n",
    "    return bleu_score\n",
    "\n",
    "# Testing with a subset\n",
    "bleu_score = calculate_bleu_score(ds[\"test\"], trainer.model, tokenizer, device, subset_size=100, batch_size=2)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "#5500it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7668"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free some memory\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIALOGUE INPUT : In a Commons debate, the ex-Labour leader said Conservative plans to limit the voting powers of Scottish MPs on Commons laws would rip up hundreds of years of parliamentary procedure. The SNP said the Conservatives wanted to create a quasi-English Parliament. But ministers said it was vital England was treated fairly as further powers were devolved to other parts of the UK. At the end of the debate, Labour staged and won a vote in which the government abstained. And Conservative MP David Davis raised a point of order to urge the government to allow more time for the matter to be considered. The government believes bills applying exclusively to England should not become law without the explicit consent of MPs from English constituencies and it wants to change Commons rules known as standing orders to give them a decisive say during their passage. Ministers say this will address the longstanding anomaly by which Scottish MPs can vote on issues such as health and education affecting England but English MPs have no say on similar matters relating to Scotland, where such policies are devolved. Mark D'Arcy, BBC Parliamentary correspondent Panic stations? From the point of view of the government whips this afternoon's emergency Commons debate on English Votes for English Laws was really rather alarming. The debate, so skilfully secured by Lib Dem ex Scottish secretary Alistair Carmichael, produced a slightly pointless vote in which the Conservatives mostly abstained, but it brought all kinds of nasty tensions to the surface. Conservative MPs are supposed to be signed up for EVEL under the terms of their manifesto, but there were clearly quite a few with doubts, some about the policy, many more about the process, which was Mr Carmichael's line of attack. Read the article in full However, the plans came under sustained attack from Labour, SNP and Lib Dems in an urgent debate, secured by former Scottish Secretary Alistair Carmichael. MPs are due to debate and vote on the government's plans next week but Mr Carmichael, the MP for Orkney and Shetland, said the plans required more detailed consideration, arguably through primary legislation. If MPs representing constituencies in England had a veto on certain legislation, it would breach the longstanding principle that MPs, no matter who they represented, were all equal. To go as far as the government wants to go in the timescale they want to go brings with it an attendant level of risk that I would consider to be irresponsible, he said. They are not entitled to use the UK Parliament as a proxy for an English Parliament. Assurances that the new system would be reviewed by MPs after a year were inadequate, he added, saying this would not be capable of putting a dangerous genie back in the bottle after it had been let out - we all know that is the political reality. But Commons Leader Chris Grayling said it was simply incorrect to claim some MPs would be prevented from debating and voting on certain legislation and would continue to exercise the same rights as they do now. Pressed by Mr Miliband on what the meaning and definition of English-only legislation would be, Mr Grayling said it would be up to the Speaker to decide but he believed a simple test would be what things were devolved to Scotland. Requiring bills to have the support of a double majority of the whole of the Commons and those MPs representing England would also help mitigate any resentment felt by English voters about the slower pace of devolution to England. It is of vital importance that English citizens of the UK, as we move to an extra layer of devolution to Scotland and Wales and devolve additional tax powers to Northern Ireland, that they think it is fair, he said. It is what we pledged to do in our manifesto. We set it out in detail, step by step by step. We are implementing these changes and keeping our promises. I think the people who elected us would expect nothing else. Mr Miliband said the Conservatives had the power to address the issue after winning the election but urged them to think again. Is this true to the traditions of Conservatism? No because the last thing you do is rip up hundreds of years of constitutional practice in a standing order vote just before the House goes into recess, he said. Doing this procedure in the way it is being proposed is an act of constitutional vandalism. It really is. The SNP said Scottish voters would be affected by legislation on schools and NHS budgets in England through the Barnett Formula used to allocate public spending to different nations of the UK. This not just English votes for English laws, this is English votes for Scottish laws, said Pete Wishart, MP for Perth and North Perthshire. It is totally and utterly unacceptable. He added: Why don't they just tattoo our foreheads 'Scottish' and then they would be able to identify us. And Conservative MP Sir Edward Leigh said the move would not make any difference to the outcome of votes and urged ministers to omit laws which indirectly applied to Scotland, saying not to do so would be a gift to the SNP's independence campaign. Responding to a question by an SNP MP on Monday, Mr Grayling said the only English-only measure during the last Parliament had been the Education Bill, and there were 13 Bills which applied to England and Wales.\n",
      "SUMMARY TRAIN (: Ed Miliband has called on the government to allow more time for MPs to consider proposals for English-only legislation.\n",
      "DIALOGUE INPUT : Mr Bondevik, who said he was travelling on his diplomatic passport, was held for about an hour. He said his passport also indicates he is a former prime minister. Immigration officials told him it was unrelated to President Trump's temporary ban on Iranian nationals. Instead, he was told it related to a 2015 law which places extra restrictions on countries that are part of the US visa waiver programme, according to an interview with ABC7 news. But Mr Bondevik said he has never had an issue travelling to the US with the same document before Mr Trump's order. During his 2014 Iran trip, he spoke against extremism at an international conference on behalf of human rights organisation The Oslo Centre, of which he is president. Iran is one of the seven countries affected by the controversial executive order from the new president. I was surprised, and I was provoked, he said, suggesting that the mention of Iran had made him stick out. There is no reason to be afraid of a former head of government who has been on official visits several times to this country, including in the White House, he told Norwegian broadcaster TV2. The former prime minister was flying to the US to attend the national prayer breakfast event in Washington - which President Donald Trump also attended.\n",
      "SUMMARY VALIDATION (: Former Norwegian President Aleksandar Bondevik has told the BBC he was detained by immigration officials in the US on Tuesday.\n",
      "DIALOGUE INPUT : Tawel Fan ward at Glan Clwyd Hospital, Denbighshire, was closed more than three years ago and a report found some patients were treated like animals. It has emerged that at least seven patients' families were told treatment may have contributed to their deaths. Betsi Cadwaladr health board said an investigation was under way. It acknowledged the quality of care provided could have been a contributory factor to the deaths of some patients. A review of mortality rates on the ward has never been published although it is understood it has been completed. Relatives of one patient told BBC Wales Today they were told medical care on the ward was inadequate. Correspondence seen by the programme included an apology from the health board to the family, who do not want to be identified. One letter said: Experts found that there were problems in the health care which may have contributed to the death. It added that the board is very much engaged in a thorough search for the truth about the Tawel Fan ward. But the family were unconvinced lessons had been learned and said questions remained unanswered and, as far as they were aware, nobody had lost their job, let alone be prosecuted. The scandal of Tawel Fan pushed the already troubled health board into close supervision by the Welsh Government. It remains in special measures which costs Â£5m a year. An initial report into what happened at Tawel Fan was published almost three years ago. Two more reports are due later this year. One of them, being compiled by the Health and Social Care Advisory Service (Hascas), is expected to include details of a mortality review of Tawel Fan patients. But Geoff Ryall-Harvey, who leads the patient watchdog Community Health Council in north Wales, said it should be released as soon as possible. It may stop this practice elsewhere, he added. A Betsi Cadwaladr health board spokesman said: We acknowledge that the quality of care provided could have been a contributory factor to the death of some patients. Whether this is the case will be established as part of the independent Hascas investigation, which is currently being carried out. In order to establish whether or not the quality of care contributed to any patients' death, every aspect of every patient's care has to be investigated. This is a complicated and time consuming process, but must be carried out in order to determine whether or not the care provided was a contributory factor to any patients' death. Every family involved in the investigation will receive an individual report detailing the care provided to their relative. These reports will also help inform the findings of the Tawel Fan investigation.\n",
      "SUMMARY TEST (: A health board has apologised to the families of patients who died after being treated like animals at a hospital.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(input_text, model, tokenizer, device):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], \n",
    "                                 max_length=161, num_beams=5, length_penalty=2.0, early_stopping=True)\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# TRAIN SAMPLE\n",
    "input_dialogue_train = \"In a Commons debate, the ex-Labour leader said Conservative plans to limit the voting powers of Scottish MPs on Commons laws would rip up hundreds of years of parliamentary procedure. The SNP said the Conservatives wanted to create a quasi-English Parliament. But ministers said it was vital England was treated fairly as further powers were devolved to other parts of the UK. At the end of the debate, Labour staged and won a vote in which the government abstained. And Conservative MP David Davis raised a point of order to urge the government to allow more time for the matter to be considered. The government believes bills applying exclusively to England should not become law without the explicit consent of MPs from English constituencies and it wants to change Commons rules known as standing orders to give them a decisive say during their passage. Ministers say this will address the longstanding anomaly by which Scottish MPs can vote on issues such as health and education affecting England but English MPs have no say on similar matters relating to Scotland, where such policies are devolved. Mark D'Arcy, BBC Parliamentary correspondent Panic stations? From the point of view of the government whips this afternoon's emergency Commons debate on English Votes for English Laws was really rather alarming. The debate, so skilfully secured by Lib Dem ex Scottish secretary Alistair Carmichael, produced a slightly pointless vote in which the Conservatives mostly abstained, but it brought all kinds of nasty tensions to the surface. Conservative MPs are supposed to be signed up for EVEL under the terms of their manifesto, but there were clearly quite a few with doubts, some about the policy, many more about the process, which was Mr Carmichael's line of attack. Read the article in full However, the plans came under sustained attack from Labour, SNP and Lib Dems in an urgent debate, secured by former Scottish Secretary Alistair Carmichael. MPs are due to debate and vote on the government's plans next week but Mr Carmichael, the MP for Orkney and Shetland, said the plans required more detailed consideration, arguably through primary legislation. If MPs representing constituencies in England had a veto on certain legislation, it would breach the longstanding principle that MPs, no matter who they represented, were all equal. To go as far as the government wants to go in the timescale they want to go brings with it an attendant level of risk that I would consider to be irresponsible, he said. They are not entitled to use the UK Parliament as a proxy for an English Parliament. Assurances that the new system would be reviewed by MPs after a year were inadequate, he added, saying this would not be capable of putting a dangerous genie back in the bottle after it had been let out - we all know that is the political reality. But Commons Leader Chris Grayling said it was simply incorrect to claim some MPs would be prevented from debating and voting on certain legislation and would continue to exercise the same rights as they do now. Pressed by Mr Miliband on what the meaning and definition of English-only legislation would be, Mr Grayling said it would be up to the Speaker to decide but he believed a simple test would be what things were devolved to Scotland. Requiring bills to have the support of a double majority of the whole of the Commons and those MPs representing England would also help mitigate any resentment felt by English voters about the slower pace of devolution to England. It is of vital importance that English citizens of the UK, as we move to an extra layer of devolution to Scotland and Wales and devolve additional tax powers to Northern Ireland, that they think it is fair, he said. It is what we pledged to do in our manifesto. We set it out in detail, step by step by step. We are implementing these changes and keeping our promises. I think the people who elected us would expect nothing else. Mr Miliband said the Conservatives had the power to address the issue after winning the election but urged them to think again. Is this true to the traditions of Conservatism? No because the last thing you do is rip up hundreds of years of constitutional practice in a standing order vote just before the House goes into recess, he said. Doing this procedure in the way it is being proposed is an act of constitutional vandalism. It really is. The SNP said Scottish voters would be affected by legislation on schools and NHS budgets in England through the Barnett Formula used to allocate public spending to different nations of the UK. This not just English votes for English laws, this is English votes for Scottish laws, said Pete Wishart, MP for Perth and North Perthshire. It is totally and utterly unacceptable. He added: Why don't they just tattoo our foreheads 'Scottish' and then they would be able to identify us. And Conservative MP Sir Edward Leigh said the move would not make any difference to the outcome of votes and urged ministers to omit laws which indirectly applied to Scotland, saying not to do so would be a gift to the SNP's independence campaign. Responding to a question by an SNP MP on Monday, Mr Grayling said the only English-only measure during the last Parliament had been the Education Bill, and there were 13 Bills which applied to England and Wales.\"  # Replace with your dialogue\n",
    "summary_train = generate_summary(input_dialogue_train, trainer.model, tokenizer, device)\n",
    "print(\"DIALOGUE INPUT :\", input_dialogue_train)\n",
    "print(\"SUMMARY TRAIN (:\", summary_train)\n",
    "\n",
    "# VALIDATION SAMPLE\n",
    "input_dialogue_val = \"Mr Bondevik, who said he was travelling on his diplomatic passport, was held for about an hour. He said his passport also indicates he is a former prime minister. Immigration officials told him it was unrelated to President Trump's temporary ban on Iranian nationals. Instead, he was told it related to a 2015 law which places extra restrictions on countries that are part of the US visa waiver programme, according to an interview with ABC7 news. But Mr Bondevik said he has never had an issue travelling to the US with the same document before Mr Trump's order. During his 2014 Iran trip, he spoke against extremism at an international conference on behalf of human rights organisation The Oslo Centre, of which he is president. Iran is one of the seven countries affected by the controversial executive order from the new president. I was surprised, and I was provoked, he said, suggesting that the mention of Iran had made him stick out. There is no reason to be afraid of a former head of government who has been on official visits several times to this country, including in the White House, he told Norwegian broadcaster TV2. The former prime minister was flying to the US to attend the national prayer breakfast event in Washington - which President Donald Trump also attended.\"  # Replace with your dialogue\n",
    "summary_val = generate_summary(input_dialogue_val, trainer.model, tokenizer, device)\n",
    "print(\"DIALOGUE INPUT :\", input_dialogue_val)\n",
    "print(\"SUMMARY VALIDATION (:\", summary_val)\n",
    "\n",
    "# TEST SAMPLE\n",
    "input_dialogue_test = \"Tawel Fan ward at Glan Clwyd Hospital, Denbighshire, was closed more than three years ago and a report found some patients were treated like animals. It has emerged that at least seven patients' families were told treatment may have contributed to their deaths. Betsi Cadwaladr health board said an investigation was under way. It acknowledged the quality of care provided could have been a contributory factor to the deaths of some patients. A review of mortality rates on the ward has never been published although it is understood it has been completed. Relatives of one patient told BBC Wales Today they were told medical care on the ward was inadequate. Correspondence seen by the programme included an apology from the health board to the family, who do not want to be identified. One letter said: Experts found that there were problems in the health care which may have contributed to the death. It added that the board is very much engaged in a thorough search for the truth about the Tawel Fan ward. But the family were unconvinced lessons had been learned and said questions remained unanswered and, as far as they were aware, nobody had lost their job, let alone be prosecuted. The scandal of Tawel Fan pushed the already troubled health board into close supervision by the Welsh Government. It remains in special measures which costs Â£5m a year. An initial report into what happened at Tawel Fan was published almost three years ago. Two more reports are due later this year. One of them, being compiled by the Health and Social Care Advisory Service (Hascas), is expected to include details of a mortality review of Tawel Fan patients. But Geoff Ryall-Harvey, who leads the patient watchdog Community Health Council in north Wales, said it should be released as soon as possible. It may stop this practice elsewhere, he added. A Betsi Cadwaladr health board spokesman said: We acknowledge that the quality of care provided could have been a contributory factor to the death of some patients. Whether this is the case will be established as part of the independent Hascas investigation, which is currently being carried out. In order to establish whether or not the quality of care contributed to any patients' death, every aspect of every patient's care has to be investigated. This is a complicated and time consuming process, but must be carried out in order to determine whether or not the care provided was a contributory factor to any patients' death. Every family involved in the investigation will receive an individual report detailing the care provided to their relative. These reports will also help inform the findings of the Tawel Fan investigation.\"  # Replace with your dialogue\n",
    "summary_test = generate_summary(input_dialogue_test, trainer.model, tokenizer, device)\n",
    "print(\"DIALOGUE INPUT :\", input_dialogue_test)\n",
    "print(\"SUMMARY TEST (:\", summary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores on Subset: {'rouge1': AggregateScore(low=Score(precision=0.4315830088241217, recall=0.3574231608428432, fmeasure=0.38385465959289244), mid=Score(precision=0.43502798335377973, recall=0.3601564009121777, fmeasure=0.3867002135077965), high=Score(precision=0.43807216319982695, recall=0.36271897967657923, fmeasure=0.3892429940535571)), 'rouge2': AggregateScore(low=Score(precision=0.18801655397960232, recall=0.154659037656873, fmeasure=0.1664991011804962), mid=Score(precision=0.19085409171130258, recall=0.15714728175533343, fmeasure=0.16907595361147953), high=Score(precision=0.19393223202573878, recall=0.15975815411422092, fmeasure=0.1717838627643787)), 'rougeL': AggregateScore(low=Score(precision=0.3514365853320121, recall=0.29149576731545773, fmeasure=0.31279052732592283), mid=Score(precision=0.3544319029068154, recall=0.29417114489588203, fmeasure=0.3154797325664329), high=Score(precision=0.3579650710201704, recall=0.29702822650632227, fmeasure=0.31852707532764757)), 'rougeLsum': AggregateScore(low=Score(precision=0.3512477774093214, recall=0.2917062131581922, fmeasure=0.31275109684209706), mid=Score(precision=0.3544523495010109, recall=0.2941245518809025, fmeasure=0.31549530228984013), high=Score(precision=0.35747397123259256, recall=0.2969397687689815, fmeasure=0.3181511312320544))}\n"
     ]
    }
   ],
   "source": [
    "def calculate_rouge_scores_subset(dataset, model, tokenizer, device, subset_size=100, batch_size=16, column_text=\"dialogue\", column_summary=\"summary\"):\n",
    "    # Select a subset of the dataset for testing\n",
    "    dataset_subset = dataset.select(range(subset_size))\n",
    "\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(0, subset_size, batch_size):\n",
    "        # Prepare batch\n",
    "        batch_texts = dataset_subset[column_text][i:i+batch_size]\n",
    "        batch_summs = dataset_subset[column_summary][i:i+batch_size]\n",
    "\n",
    "        # Tokenize inputs and generate summaries\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=161, num_beams=5)\n",
    "\n",
    "        # Decode summaries\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
    "\n",
    "        # Add to references and candidates\n",
    "        candidates.extend(decoded_summaries)\n",
    "        references.extend([[ref.split()] for ref in batch_summs])\n",
    "\n",
    "    # Ensure equal number of candidates and references\n",
    "    assert len(candidates) == len(references), \"Number of candidates and references must be equal\"\n",
    "\n",
    "    # Calculating BLEU score\n",
    "    rouge_scores = rouge.compute(predictions=candidates, references=references)\n",
    "    return rouge_scores\n",
    "\n",
    "# Testing with a subset\n",
    "rouge_scores_subset = calculate_rouge_scores_subset(ds[\"test\"], trainer.model, tokenizer, device, subset_size=100, batch_size=2)\n",
    "print(\"ROUGE Scores on Subset:\", rouge_scores_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "136fb8d4c2514d9c809673cfba19c83d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da99480d348b43c5af4594adb4eec1f1",
      "max": 11327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d3db4f2f030c46e3b2aad44bc4cd1a34",
      "value": 11327
     }
    },
    "1a482faee07a4491ab496529f076ab79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f207af3c0e64e8ebdc50a60249a5833",
      "placeholder": "​",
      "style": "IPY_MODEL_3a6bf2695aa14f46aabce90aa615329a",
      "value": " 11327/11327 [00:19&lt;00:00, 583.41 examples/s]"
     }
    },
    "3a6bf2695aa14f46aabce90aa615329a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a75d6018ef748bbb0fbde3c7e9d037b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c58c819cec9467390979f88e0b83c41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c42dc52891394d828b78bdfc1400a9e6",
       "IPY_MODEL_136fb8d4c2514d9c809673cfba19c83d",
       "IPY_MODEL_1a482faee07a4491ab496529f076ab79"
      ],
      "layout": "IPY_MODEL_e61e186e6b75440f93564cdd68c7c873"
     }
    },
    "6f207af3c0e64e8ebdc50a60249a5833": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c2d88f26ba44cd0923d47e7d63a80a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c42dc52891394d828b78bdfc1400a9e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a75d6018ef748bbb0fbde3c7e9d037b",
      "placeholder": "​",
      "style": "IPY_MODEL_8c2d88f26ba44cd0923d47e7d63a80a9",
      "value": "Map: 100%"
     }
    },
    "d3db4f2f030c46e3b2aad44bc4cd1a34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da99480d348b43c5af4594adb4eec1f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e61e186e6b75440f93564cdd68c7c873": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
